set.seed(76)
ggpairs(training, columns = c("bone_length", "rotting_flesh", "hair_length", "has_soul")
ggpairs(training, columns = c("bone_length", "rotting_flesh", "hair_length", "has_soul"))
ggpairs(training, columns = c("bone_length", "rotting_flesh", "hair_length", "has_soul"))
ggpairs(training, columns = c("bone_length", "rotting_flesh", "hair_length", "has_soul"))
View(training)
ggplot(data = training,
aes(x = hair_length, y = type)) +
geom_point(color = type)
ggplot(data = training,
aes(x = hair_length, y = type)) +
geom_point(aes(color = type))
ggplot(data = training,
aes(x = hair_length, y = type)) +
geom_boxplot()
ggplot(data = training,
aes(x = type, y = hair_color)) +
geom_boxplot()
ggplot(data = training,
aes(x = type, y = hair_length)) +
geom_boxplot()
# Load all your packages here:
library(tidyverse)
library(GGally)
library(rpart)
# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
echo = TRUE, warning = FALSE, message = FALSE,
fig.width = 16/2, fig.height = 9/2
)
# Set seed value of random number generator here. This is in order to get
# "replicable" randomness, so that any results based on random sampling or
# resampling are replicable everytime you knit this file. Why use a seed value
# of 76? For no other reason than 76 is one of my favorite numbers:
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
ggplot(data = training,
aes(x = color)) +
geom_bar()
ggplot(data = training,
aes(x = type)) +
geom_bar()
ggpairs(training, columns = c("bone_length", "rotting_flesh", "hair_length", "has_soul"))
ggplot(data = training,
aes(x = type, y = hair_length)) +
geom_boxplot()
View(training)
model_1_formula <- as.formula(type~bone_length + rotting_flesh + hair_length + has_soul)
tree_parameter <- rpart.control(maxdepth = 5)
model_1_CART <- rpart(model_1_formula, data = training, control = tree_parameter)
model_1_formula <- as.formula(type~bone_length + rotting_flesh + hair_length + has_soul)
tree_parameter <- rpart.control(maxdepth = 5)
model_1_CART <- rpart(model_1_formula, data = training, control = tree_parameter)
plot(model_1_CART)
text(model_1_CART, use.n = TRUE)
title("Predicting monster type with bone length, rotting flesh, hair length and soul")
box()
View(training)
View(model_1_CART)
View(sample_submission)
p_hat_matrix_train <- model_1_CART %>%
predict(type = "prob", newdata = training) %>%
as_tibble()
View(p_hat_matrix_train)
p_hat_matrix_train <- model_1_CART %>%
predict(type = "class", newdata = training) %>%
as_tibble()
p_hat_matrix_train <- model_1_CART %>%
predict(type = "class", newdata = training) %>%
as_tibble()
p_hat_matrix_train <- model_1_CART %>%
predict(type = "class", newdata = training) %>%
enframe()
View(p_hat_matrix_train)
View(training)
training <- training %>%
mutate(type_hat = p_hat_matrix_train$value)
View(training)
sum(training$type == training$type_hat)
sum(training$type == training$type_hat) / nrow(training)
accuracy = sum(training$type == training$type_hat) / nrow(training)
accuracy
alpha_df <- tibble(
alpha = seq(from = 0, to = 0.05, length = 100),
accuracy = 0
)
View(alpha_df)
?rpart.control
alpha_df <- tibble(
alpha = seq(from = 0, to = 0.05, length = 100),
accuracy = 0
)
set.seed(76)
#5 Fold Cross Validation
training <- training %>%
sample_frac(1) %>%
mutate(fold = rep(1:5, length=n())) %>%
arrange(fold)
for (i in 1:nrow(alpha_df)){
alpha <- alpha_df$alpha[i]
ALPHA <- rep(0,5)
for(j in 1:5){
pretend_training <- training %>%
filter(fold != j)
pretend_test <- training %>%
filter (fold == j)
#Fitting the Model + Training Model
model_1_formula <- as.formula(type~bone_length + rotting_flesh + hair_length + has_soul)
tree_parameter <- rpart.control(maxdepth = 5, cp = alpha)
model_1_CART <- rpart(model_1_formula, data = pretend_training, control = tree_parameter)
#Prediction based on model
p_hat_matrix_train <- model_1_CART %>%
predict(type = "class", newdata = pretend_test) %>%
enframe()
pretend_test <- pretend_test %>%
mutate(type_hat = p_hat_matrix_train$value)
accuracy = sum(pretend_test$type == pretend_test$type_hat) / nrow(pretend_test)
ALPHA[j] <- accuracy
}
alpha_df$accuracy[i] <- mean(ALPHA)
}
# Load all your packages here:
library(tidyverse)
library(GGally)
library(rpart)
# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
echo = TRUE, warning = FALSE, message = FALSE,
fig.width = 16/2, fig.height = 9/2
)
# Set seed value of random number generator here. This is in order to get
# "replicable" randomness, so that any results based on random sampling or
# resampling are replicable everytime you knit this file. Why use a seed value
# of 76? For no other reason than 76 is one of my favorite numbers:
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
alpha_df <- tibble(
alpha = seq(from = 0, to = 0.05, length = 100),
accuracy = 0
)
set.seed(76)
#5 Fold Cross Validation
training <- training %>%
sample_frac(1) %>%
mutate(fold = rep(1:5, length=n())) %>%
arrange(fold)
for (i in 1:nrow(alpha_df)){
alpha <- alpha_df$alpha[i]
ALPHA <- rep(0,5)
for(j in 1:5){
pretend_training <- training %>%
filter(fold != j)
pretend_test <- training %>%
filter (fold == j)
#Fitting the Model + Training Model
model_1_formula <- as.formula(type~bone_length + rotting_flesh + hair_length + has_soul)
tree_parameter <- rpart.control(maxdepth = 5, cp = alpha)
model_1_CART <- rpart(model_1_formula, data = pretend_training, control = tree_parameter)
#Prediction based on model
p_hat_matrix_train <- model_1_CART %>%
predict(type = "class", newdata = pretend_test) %>%
enframe()
pretend_test <- pretend_test %>%
mutate(type_hat = p_hat_matrix_train$value)
accuracy = sum(pretend_test$type == pretend_test$type_hat) / nrow(pretend_test)
ALPHA[j] <- accuracy
}
alpha_df$accuracy[i] <- mean(ALPHA)
}
alpha_df <- tibble(
alpha = seq(from = 0, to = 0.05, length = 100),
accuracy = 0
)
set.seed(76)
#5 Fold Cross Validation
training <- training %>%
sample_frac(1) %>%
mutate(fold = rep(1:5, length=n())) %>%
arrange(fold)
for (i in 1:nrow(alpha_df)){
alpha <- alpha_df$alpha[i]
ALPHA <- rep(0,5)
for(j in 1:5){
pretend_training <- training %>%
filter(fold != j)
pretend_test <- training %>%
filter (fold == j)
#Fitting the Model + Training Model
model_1_formula <- as.formula(type~bone_length + rotting_flesh + hair_length + has_soul)
tree_parameter <- rpart.control(maxdepth = 5, cp = alpha)
model_1_CART <- rpart(model_1_formula, data = pretend_training, control = tree_parameter)
#Prediction based on model
p_hat_matrix_train <- model_1_CART %>%
predict(type = "class", newdata = pretend_test) %>%
enframe()
pretend_test <- pretend_test %>%
mutate(type_hat = p_hat_matrix_train$value)
accuracy = sum(pretend_test$type == pretend_test$type_hat) / nrow(pretend_test)
ALPHA[j] <- accuracy
}
alpha_df$accuracy[i] <- mean(ALPHA)
}
#Plotting the relationship between alpha complexity parameter and accuracy
ggplot(alpha_df, aes(x = alpha, y = accuracy)) +
geom_line()+
labs(title = "Relationship between the alpha complexity parameter and accuracy")
max(alpha_df$accuracy)
which.max(alpha_df$accuracy)
alpha_df <- tibble(
alpha = seq(from = 0, to = 0.05, length = 100),
accuracy = 0
)
#5 Fold Cross Validation
training <- training %>%
sample_frac(1) %>%
mutate(fold = rep(1:5, length=n())) %>%
arrange(fold)
for (i in 1:nrow(alpha_df)){
alpha <- alpha_df$alpha[i]
ALPHA <- rep(0,5)
for(j in 1:5){
pretend_training <- training %>%
filter(fold != j)
pretend_test <- training %>%
filter (fold == j)
#Fitting the Model + Training Model
model_1_formula <- as.formula(type~bone_length + rotting_flesh + hair_length + has_soul)
tree_parameter <- rpart.control(maxdepth = 5, cp = alpha)
model_1_CART <- rpart(model_1_formula, data = pretend_training, control = tree_parameter)
#Prediction based on model
p_hat_matrix_train <- model_1_CART %>%
predict(type = "class", newdata = pretend_test) %>%
enframe()
pretend_test <- pretend_test %>%
mutate(type_hat = p_hat_matrix_train$value)
accuracy = sum(pretend_test$type == pretend_test$type_hat) / nrow(pretend_test)
ALPHA[j] <- accuracy
}
alpha_df$accuracy[i] <- mean(ALPHA)
}
#Which alpha value yields highest accuracy
alpha_df[which.max(alpha_df$accuracy),]
#Which alpha value yields highest accuracy
optimal_alpha <- alpha_df[which.max(alpha_df$accuracy),]
View(optimal_alpha)
#Which alpha value yields highest accuracy
best_alpha_df <- alpha_df[which.max(alpha_df$accuracy),]
best_alpha = optimal_alpha$accuracy
best_alpha
best_alpha = optimal_alpha$alpha
model_2_formula <- as.formula(type~bone_length + rotting_flesh + hair_length + has_soul)
tree_parameter <- rpart.control(maxdepth = 5, cp = best_alpha)
model_2_CART <- rpart(model_1_formula, data = training, control = tree_parameter)
#Prediction based on model
p_hat_matrix_train1 <- model_2_CART %>%
predict(type = "class", newdata = test) %>%
enframe()
test <- test %>%
mutate(type_hat = p_hat_matrix_train$value)
#Prediction based on model
predict_test <- model_2_CART %>%
predict(type = "class", newdata = test) %>%
enframe()
test <- test %>%
mutate(type_hat = predict_test$value)
View(test)
test <- test %>%
mutate(type = predict_test$value)
submission <- test %>%
select(id, type)
View(submission)
training$color
ggplot(data = training,
aes(x = color)) +
geom_bar()
training2 <- as.data.table(training)
library(mltools)
install.packages(mltools)
install.packages("mltools")
library(data.table)
training2 <- as.data.table(training)
training2 <- one_hot(training2)
training2 <- onehot(training2)
training2 <- onehot(training)
install.packages("onehot")
library(onehot)
training2 <- onehot(training)
View(training2)
?onehot
training2 <- onehot(training, stringsAsFactors = TRUE)
View(training)
training %>%
mutate(value = 1)  %>%
spread(color, value,  fill = 0 )
training <- training %>%
mutate(value = 1)  %>%
spread(color, value,  fill = 0 )
View(training)
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
training <- training %>%
mutate(value = 1)  %>%
spread(color, value,  fill = 0 )
model_3_formula <- as.formula(type ~ colorblack + colorblood + colorblue + colorclear + colorgreen + colorwhite)    tree_parameter <- rpart.control(maxdepth = 5)
model_3_formula <- as.formula(type ~ black + blood + blue + clear + green + white)
tree_parameter <- rpart.control(maxdepth = 5)
model_3_CART <- rpart(model_3_formula, data = training, control = tree_parameter)
plot(model_3_CART)
text(model_3_CART, use.n = TRUE)
title("Predicting monster type with color")
ggplot(data = training,
aes(x = color)) +
geom_bar(aes(type))
ggplot(data = training,
aes(x = type)) +
geom_bar(aes(type))
ggplot(data = training,
aes(x = color)) +
geom_bar(aes(type))
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
ggplot(data = training,
aes(x = color)) +
geom_bar(aes(type))
ggplot(data = training,
aes(x = color)) +
geom_bar()
ggplot(data = training,
aes(x = color, fill = factor(type))) +
geom_bar()
ggplot(data = training,
aes(x = color)) +
geom_bar() +
ggtitle("Count monsters by color")
ggplot(data = training,
aes(x = color, fill = as.factor(type))) +
geom_bar() +
ggtitle("Count monsters by color and type")
ggplot(data = training,
aes(x = color, fill = as.factor(type))) +
geom_bar() +
ggtitle("Count monsters by Color and Type") +
labs(fill = "Monster Type")
ggplot(data = training,
aes(x = color)) +
geom_bar() +
ggtitle("Count of Monsters by Color")
ggplot(data = training,
aes(x = type, y = hair_length)) +
geom_boxplot()
ggplot(data = training,
aes(x = type, y = hair_length)) +
geom_boxplot() +
ggtitle("Type of Monster and Hair Length")
write.csv(submission, file = "data/submission")
best_alpha = alpha_df$alpha
best_alpha = best_alpha_df$alpha
plot(model_3_CART)
text(model_3_CART, use.n = TRUE)
title("Predicting monster type with color")
box()
plot(model_3_CART)
text(model_3_CART, use.n = TRUE)
title("Predicting monster type with color")
?box
plot(model_3_CART)
text(model_3_CART, use.n = TRUE)
title("Predicting monster type with color")
box(lwd=3)
plot(model_3_CART)
text(model_3_CART, use.n = TRUE)
title("Predicting monster type with color")
accuracy = sum(training$type == training$type_hat) / nrow(training)
accuracy
# Chunk 1: setup
# Load all your packages here:
library(tidyverse)
library(GGally)
library(rpart)
# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
echo = TRUE, warning = FALSE, message = FALSE,
fig.width = 16/2, fig.height = 9/2
)
# Set seed value of random number generator here. This is in order to get
# "replicable" randomness, so that any results based on random sampling or
# resampling are replicable everytime you knit this file. Why use a seed value
# of 76? For no other reason than 76 is one of my favorite numbers:
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
# Chunk 2
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
# Chunk 4
ggplot(data = training,
aes(x = color)) +
geom_bar() +
ggtitle("Count of Monsters by Color")
# Chunk 5
ggplot(data = training,
aes(x = color, fill = as.factor(type))) +
geom_bar() +
ggtitle("Count monsters by Color and Type") +
labs(fill = "Monster Type")
# Chunk 6
ggplot(data = training,
aes(x = type)) +
geom_bar() +
ggtitle ("Counts of Monsters by type")
# Chunk 7
ggpairs(training, columns = c("bone_length", "rotting_flesh", "hair_length", "has_soul"))
# Chunk 8
ggplot(data = training,
aes(x = type, y = hair_length)) +
geom_boxplot() +
ggtitle("Type of Monster and Hair Length")
# Chunk 9
model_1_formula <- as.formula(type~bone_length + rotting_flesh + hair_length + has_soul)
tree_parameter <- rpart.control(maxdepth = 5)
model_1_CART <- rpart(model_1_formula, data = training, control = tree_parameter)
plot(model_1_CART)
text(model_1_CART, use.n = TRUE)
title("Predicting monster type with bone length, rotting flesh, hair length and soul")
box()
# Chunk 10
predict_train <- model_1_CART %>%
predict(type = "class", newdata = training) %>%
enframe()
training <- training %>%
mutate(type_hat = predict_train$value)
# Chunk 11
accuracy = sum(training$type == training$type_hat) / nrow(training)
accuracy
# Chunk 12
alpha_df <- tibble(
alpha = seq(from = 0, to = 0.05, length = 100),
accuracy = 0
)
#5 Fold Cross Validation
training <- training %>%
sample_frac(1) %>%
mutate(fold = rep(1:5, length=n())) %>%
arrange(fold)
for (i in 1:nrow(alpha_df)){
alpha <- alpha_df$alpha[i]
ALPHA <- rep(0,5)
for(j in 1:5){
pretend_training <- training %>%
filter(fold != j)
pretend_test <- training %>%
filter (fold == j)
#Fitting the Model + Training Model
tree_parameter <- rpart.control(maxdepth = 5, cp = alpha)
model_1_CART <- rpart(model_1_formula, data = pretend_training, control = tree_parameter)
#Prediction based on model
predict_pretend_test <- model_1_CART %>%
predict(type = "class", newdata = pretend_test) %>%
enframe()
pretend_test <- pretend_test %>%
mutate(type_hat = predict_pretend_test$value)
accuracy = sum(pretend_test$type == pretend_test$type_hat) / nrow(pretend_test)
ALPHA[j] <- accuracy
}
alpha_df$accuracy[i] <- mean(ALPHA)
}
# Chunk 13
#Plotting the relationship between alpha complexity parameter and accuracy
ggplot(alpha_df, aes(x = alpha, y = accuracy)) +
geom_line()+
labs(title = "Relationship between the alpha complexity parameter and accuracy")
# Chunk 14
#Which alpha value yields highest accuracy
best_alpha_df <- alpha_df[which.max(alpha_df$accuracy),]
best_alpha = best_alpha_df$alpha
# Chunk 15
model_2_formula <- as.formula(type~bone_length + rotting_flesh + hair_length + has_soul)
tree_parameter <- rpart.control(maxdepth = 5, cp = best_alpha)
model_2_CART <- rpart(model_1_formula, data = training, control = tree_parameter)
# Chunk 16
#Prediction based on model
predict_test <- model_2_CART %>%
predict(type = "class", newdata = test) %>%
enframe()
test <- test %>%
mutate(type = predict_test$value)
# Chunk 17
submission <- test %>%
select(id, type)
write.csv(submission, file = "data/submission")
# Chunk 18
library(onehot)
training2 <- onehot(training, stringsAsFactors = TRUE)
training <- training %>%
mutate(value = 1)  %>%
spread(color, value,  fill = 0 )
model_3_formula <- as.formula(type ~ black + blood + blue + clear + green + white)
tree_parameter <- rpart.control(maxdepth = 5)
model_3_CART <- rpart(model_3_formula, data = training, control = tree_parameter)
# Chunk 19
plot(model_3_CART)
text(model_3_CART, use.n = TRUE)
title("Predicting monster type with color")
accuracy
